{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural network model for the saucer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten,Conv2D,MaxPool2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an image and get its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img('saucer_data/train/normal/normal_3.jpg')\n",
    "plt.imshow(img)\n",
    "print(img.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator(rescale= 1/255)\n",
    "validation = ImageDataGenerator(rescale= 1/255)\n",
    "train_dataset = train.flow_from_directory('saucer_data/train',target_size=(168,168),batch_size=24)\n",
    "validation_dataset = train.flow_from_directory('saucer_data/validation',target_size=(168,168),batch_size=24)\n",
    "print(train_dataset.class_indices)\n",
    "print(validation_dataset.class_indices)\n",
    "pred_dict = train_dataset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16,(3,3),activation='relu',input_shape=(168,168,3)))\n",
    "print(model.output_shape)\n",
    "model.add(MaxPool2D(2,2))\n",
    "print(model.output_shape)\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(MaxPool2D(2,2))\n",
    "print(model.output_shape)\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(MaxPool2D(2,2))\n",
    "print(model.output_shape)\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(MaxPool2D(2,2))\n",
    "print(model.output_shape)\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(Dense(512,activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit =model.fit(train_dataset,epochs=10,validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the models metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(2,1) #Creates 2 subplots under 1 column\n",
    "\n",
    "#Assigning the first subplot to graph training loss and validation loss\n",
    "ax[0].plot(model.history.history['loss'],color='g',label='Training Loss')\n",
    "ax[0].plot(model.history.history['val_loss'],color='m',label='Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "#Plotting the training accuracy and validation accuracy\n",
    "ax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\n",
    "ax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model using testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val):\n",
    "    for key, value in pred_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'saucer_data/test'\n",
    "for i in os.listdir(dir_path):\n",
    "    print(i)\n",
    "    img = image.load_img(dir_path+'//'+i)\n",
    "    img = img.resize(((168,168)))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    X =  image.img_to_array(img)\n",
    "    X = np.expand_dims(X,axis=0)\n",
    "    images = np.vstack([X])\n",
    "    val = model.predict_classes(images)\n",
    "    print(get_key(val[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model to json file and save its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
